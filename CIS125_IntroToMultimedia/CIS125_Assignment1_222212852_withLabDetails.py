# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GLpao74FISI3oXmZi8kI0Nj9fpylfel2

Solution of CIS125 Assignment 1
SID: s22212852
Name: Cheung Siu Chun
"""

# importing useful functions
from math import log2
from collections import Counter

def calculate_entropy(message):
  freq_dict = Counter(message)
  M = len(message)
  H = sum(f/M * log2(M/f) for f in freq_dict.values())
  return H

message = "aaaaaaaabbbbccde"
print(f"The message {message} has an entropy of {calculate_entropy(message)}")

message = "aabb"
print(f"The message {message} has an entropy of {calculate_entropy(message)}")

message = "abbb"
print(f"The message {message} has an entropy of {calculate_entropy(message)}")

message = "aabbaacc"
print(f"The message {message} has an entropy of {calculate_entropy(message)}")

message = "I have an apple."
print(f"The message {message} has an entropy of {calculate_entropy(message)}")

"""Lab details"""

# CIS125 Lab
# Calculate the Entropy  
message = "aaaaaaaabbbbccde"
print(message)

from collections import Counter
freq_dict=Counter(message)
print(freq_dict)

M = len(message)
print("The length of message: "+ str(M))

# for calculation of probability of occurence
for s in freq_dict:
  print(f"The probability of occurence of {s}: {freq_dict[s]/M}")

# for calculating the self-info of each symbol
from math import log
for s in freq_dict:
  print(f"The self-information of {s}: {-log(freq_dict[s]/M,2)}")

# for calculation of Entropy
H = 0
for f in freq_dict.values():
  H += f/M * (log(M/f, 2))

print(f"The entropy of message '{message}': {H}")